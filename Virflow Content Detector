> Content Theft Detector (showcase)

Small, ethical showcase of a content-theft detection workflow:
- Query web results (Google Custom Search API) for possible reposts using search patterns / dorks.
- Fetch candidate pages and extract plain text.
- Compute similarity against a source text using difflib and SHA-256 fingerprints.
- Respect robots.txt, API usage limits, and copyright/TOS.

This is a *demonstration* do not use for large-scale scraping. Keys and production configs are intentionally excluded.


# detect_reposts.py
"""
Simple, ethical demo of a content-repost detector.
Uses Google Custom Search API to find candidate pages,
fetches them, extracts text, and computes similarity vs source.
"""

import os
import time
import hashlib
import requests
from bs4 import BeautifulSoup
from difflib import SequenceMatcher
from urllib.parse import urlparse
from dotenv import load_dotenv

load_dotenv()  # expects GOOGLE_API_KEY and GOOGLE_CX in .env (not committed)

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CX = os.getenv("GOOGLE_CX")
USER_AGENT = "James-Olson-ContentDetector/1.0 (+mailto:your@email)"

RATE_LIMIT_SECONDS = 1.5  # be polite

def sha256_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

def similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, a, b).ratio()

def google_search(query: str, num=5):
    if not GOOGLE_API_KEY or not GOOGLE_CX:
        raise RuntimeError("Set GOOGLE_API_KEY and GOOGLE_CX in .env")
    url = "https://www.googleapis.com/customsearch/v1"
    params = {"key": GOOGLE_API_KEY, "cx": GOOGLE_CX, "q": query, "num": num}
    resp = requests.get(url, params=params, headers={"User-Agent": USER_AGENT}, timeout=10)
    resp.raise_for_status()
    data = resp.json()
    items = data.get("items", [])
    for it in items:
        yield it.get("link")
    time.sleep(RATE_LIMIT_SECONDS)

def fetch_text(url: str) -> str:
    headers = {"User-Agent": USER_AGENT}
    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    # Simple heuristic: gather main textual nodes, strip scripts/styles
    for s in soup(["script", "style", "noscript"]):
        s.extract()
    texts = soup.get_text(separator="\n")
    # Basic cleanup
    lines = [ln.strip() for ln in texts.splitlines() if ln.strip()]
    return "\n".join(lines[:2000])  # cap length to avoid huge downloads

def detect_reposts(source_text: str, query_patterns: list):
    source_hash = sha256_text(source_text)
    results = []
    for pat in query_patterns:
        for url in google_search(pat, num=5):
            try:
                text = fetch_text(url)
            except Exception as e:
                print(f"[WARN] failed to fetch {url}: {e}")
                continue
            sim = similarity(source_text[:1000], text[:1000])  # compare prefixes for speed
            url_hash = sha256_text(text)
            results.append({"url": url, "similarity": sim, "url_hash": url_hash})
            # be polite between page fetches
            time.sleep(RATE_LIMIT_SECONDS)
    # sort by highest similarity
    results.sort(key=lambda r: r["similarity"], reverse=True)
    return source_hash, results

if __name__ == "__main__":
    # Example usage - DO NOT hardcode keys or real copyrighted text
    sample = """This is a short excerpt of an article I own. Replace with a small sample or excerpt only."""
    patterns = [
        '"This is a short excerpt of an article I own."',
        'site:medium.com "excerpt of an article"'
    ]
    source_hash, hits = detect_reposts(sample, patterns)
    print("Source hash:", source_hash)
    for h in hits[:10]:
        print(f"{h['similarity']:.3f} {h['url']} {h['url_hash'][:8]}...")
